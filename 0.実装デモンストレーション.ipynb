{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuracode/hsprogramming/blob/main/0.%E5%AE%9F%E8%A3%85%E3%83%87%E3%83%A2%E3%83%B3%E3%82%B9%E3%83%88%E3%83%AC%E3%83%BC%E3%82%B7%E3%83%A7%E3%83%B3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUI5hEP6OC7h"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchvision -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "9d4b320ad848455b9d3abe8bf571f28e",
            "1f6d5fe4efcb47688c1c27b6976482b8",
            "9576e8548b364f0eae4d3999e71fff9e",
            "61b2481195be47ec81b10b9276b590c1",
            "f71b07299b324fd8bd9e97e7cccd6c73",
            "c8299b142a7648c1905e33fafa213b76",
            "4e71857b892448389e9aabf3c0554bda",
            "187cea5f0a3142e89c7d9e54eac727fc",
            "67754df303e34b69b69e12042dc99a4d",
            "0e8aba57a04844c8af614b7a51091c01",
            "c976e311e30b428587e7e2d79e208f9a",
            "9baf03f14533451cbb568214cdf622d6",
            "4635b75ca90e4e529773ce419da5c217",
            "f2d81490f3cf4d59be658ebe86ebbc24",
            "148007a9fa9546dab928a2cc45526812"
          ]
        },
        "id": "OeVSKD2_PDhM",
        "outputId": "9e59ab52-79e6-4df1-f079-af989cf0ad37"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d4b320ad848455b9d3abe8bf571f28e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model_index.json:   0%|          | 0.00/543 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f6d5fe4efcb47688c1c27b6976482b8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9576e8548b364f0eae4d3999e71fff9e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/807 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61b2481195be47ec81b10b9276b590c1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f71b07299b324fd8bd9e97e7cccd6c73",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "scheduler_config.json:   0%|          | 0.00/346 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8299b142a7648c1905e33fafa213b76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4e71857b892448389e9aabf3c0554bda",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "187cea5f0a3142e89c7d9e54eac727fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67754df303e34b69b69e12042dc99a4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "text_encoder/model.safetensors:   0%|          | 0.00/1.36G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e8aba57a04844c8af614b7a51091c01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c976e311e30b428587e7e2d79e208f9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/553 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9baf03f14533451cbb568214cdf622d6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/911 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4635b75ca90e4e529773ce419da5c217",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unet/diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.46G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2d81490f3cf4d59be658ebe86ebbc24",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vae/diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "148007a9fa9546dab928a2cc45526812",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://fe0d1742b18c821328.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://fe0d1742b18c821328.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆåˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰\n",
        "pipe = DiffusionPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\")\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "def generate_image(prompt):\n",
        "    image = pipe(prompt).images[0]\n",
        "    return image\n",
        "\n",
        "# Gradioã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã®ä½œæˆ\n",
        "iface = gr.Interface(fn=generate_image,\n",
        "                     inputs=\"text\",\n",
        "                     outputs=\"image\",\n",
        "                     title=\"AIã‚¤ãƒ©ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼\",\n",
        "                     description=\"å¥½ããªè¨€è‘‰ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰ã‚’å…¥åŠ›ã—ã¦ã€ç”»åƒã‚’ä½œã£ã¦ã¿ã‚ˆã†ï¼\")\n",
        "iface.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "72FDbzfyNxmO",
        "outputId": "a3b4b4b8-4aa0-4f2e-9df5-8f7f8859e7ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 165MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n",
            "\n",
            "ã‚‚ã— 'torch' ã‚„ 'torchvision' ãŒãªã„ã¨ã„ã†ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰ã€!pip install torch torchvision ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d6a60249bebf9bbee9.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://d6a60249bebf9bbee9.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã™ã¹ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import gradio as gr\n",
        "from PIL import Image, ImageFilter, ImageDraw\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.models import resnet50\n",
        "import requests\n",
        "\n",
        "# --- AIãƒ¢ãƒ‡ãƒ«ã¨ãƒ©ãƒ™ãƒ«ã®æº–å‚™ ---\n",
        "# ã“ã®æº–å‚™ã¯ä¸€åº¦ã ã‘è¡Œã„ã¾ã™\n",
        "try:\n",
        "    model = resnet50(pretrained=True)\n",
        "    model.eval()\n",
        "\n",
        "    response = requests.get(\"https://git.io/JJkYN\")\n",
        "    labels = response.text.split(\"\\n\")\n",
        "    print(\"AIãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "except Exception as e:\n",
        "    print(f\"AIãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
        "    model = None\n",
        "    labels = None\n",
        "\n",
        "# --- 1. ç”»åƒåŠ å·¥ã®ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆå‰å›ã®ã‚³ãƒ¼ãƒ‰ã¨åŒã˜ï¼‰ ---\n",
        "def image_processing(img, filter_type, blur_level, watermark_text):\n",
        "    if img is None:\n",
        "        return None, \"ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\"\n",
        "    processed_img = img.copy()\n",
        "    if filter_type == \"ç™½é»’\":\n",
        "        processed_img = processed_img.convert('L')\n",
        "    elif filter_type == \"90åº¦å›è»¢\":\n",
        "        processed_img = processed_img.rotate(90, expand=True)\n",
        "    if blur_level > 0:\n",
        "        processed_img = processed_img.filter(ImageFilter.GaussianBlur(radius=blur_level))\n",
        "    if watermark_text:\n",
        "        draw = ImageDraw.Draw(processed_img)\n",
        "        draw.text((10, 10), watermark_text, fill=\"white\", stroke_width=1, stroke_fill=\"black\")\n",
        "    return processed_img, f\"å‡¦ç†å®Œäº†ï¼š{filter_type}\"\n",
        "\n",
        "# --- 2. AIã«ã‚ˆã‚‹ç”»åƒåˆ†æã®ãƒ­ã‚¸ãƒƒã‚¯ ---\n",
        "# (æ•™æã®ã‚³ãƒ¼ãƒ‰ã‚’é–¢æ•°ã¨ã—ã¦æ•´ç†)\n",
        "@torch.no_grad()\n",
        "def predict(input_img):\n",
        "    if model is None or labels is None or input_img is None:\n",
        "        return \"AIãƒ¢ãƒ‡ãƒ«ãŒæº–å‚™ã§ãã¦ã„ãªã„ã‹ã€ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚\"\n",
        "\n",
        "    # PIL Imageã‚’PyTorchãŒæ‰±ãˆã‚‹Tensorã«å¤‰æ›\n",
        "    img = F.resize(input_img, (224, 224))\n",
        "    img = F.to_tensor(img)\n",
        "    if img.shape[0] == 1: # ç™½é»’ç”»åƒã®å ´åˆ\n",
        "        img = img.repeat(3, 1, 1)\n",
        "    img = F.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "    # æ¨è«–ã®å®Ÿè¡Œ\n",
        "    output = model(img).squeeze(0)\n",
        "    probs = torch.nn.functional.softmax(output, dim=0).numpy()\n",
        "\n",
        "    # çµæœã‚’{ãƒ©ãƒ™ãƒ«: ç¢ºç‡}ã®è¾æ›¸å½¢å¼ã§è¿”ã™\n",
        "    return {labels[i]: float(probs[i]) for i in range(1000)}\n",
        "\n",
        "# --- Gradioã®UIã‚’æ§‹ç¯‰ ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# ç·åˆç”»åƒå‡¦ç†ã‚¢ãƒ—ãƒª\")\n",
        "    gr.Markdown(\"æ‰‹å‹•ã§ã®ç”»åƒåŠ å·¥ã¨ã€AIã«ã‚ˆã‚‹ç”»åƒåˆ†æãŒã§ãã¾ã™ã€‚\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # --- ã‚¿ãƒ–1: ç”»åƒåŠ å·¥ ---\n",
        "        with gr.TabItem(\"ç”»åƒåŠ å·¥\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    proc_input = gr.Image(type=\"pil\", label=\"åŠ å·¥ã—ãŸã„ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\")\n",
        "                    filter_radio = gr.Radio([\"ãªã—\", \"ç™½é»’\", \"90åº¦å›è»¢\"], label=\"ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é¸æŠ\", value=\"ãªã—\")\n",
        "                    blur_slider = gr.Slider(0, 10, value=0, label=\"ã¼ã‹ã—ã®å¼·ã•\")\n",
        "                    watermark_input = gr.Textbox(label=\"é€ã‹ã—æ–‡å­—\")\n",
        "                    proc_btn = gr.Button(\"ç”»åƒå‡¦ç†ã‚’å®Ÿè¡Œ\")\n",
        "                with gr.Column():\n",
        "                    proc_output = gr.Image(label=\"åŠ å·¥å¾Œã®ç”»åƒ\")\n",
        "                    proc_status = gr.Textbox(label=\"å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\")\n",
        "            proc_btn.click(\n",
        "                fn=image_processing,\n",
        "                inputs=[proc_input, filter_radio, blur_slider, watermark_input],\n",
        "                outputs=[proc_output, proc_status]\n",
        "            )\n",
        "\n",
        "        # --- ã‚¿ãƒ–2: AIç”»åƒåˆ†æ ---\n",
        "        with gr.TabItem(\"AIç”»åƒåˆ†æ\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    ai_input = gr.Image(type=\"pil\", label=\"åˆ†æã—ãŸã„ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\")\n",
        "                    ai_btn = gr.Button(\"AIåˆ†æã‚’å®Ÿè¡Œ\")\n",
        "                with gr.Column():\n",
        "                    ai_output = gr.Label(num_top_classes=5, label=\"åˆ†æçµæœ\")\n",
        "            ai_btn.click(\n",
        "                fn=predict,\n",
        "                inputs=ai_input,\n",
        "                outputs=ai_output\n",
        "            )\n",
        "\n",
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ä¿ƒã™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
        "print(\"\\nã‚‚ã— 'torch' ã‚„ 'torchvision' ãŒãªã„ã¨ã„ã†ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰ã€!pip install torch torchvision ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
        "# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "ded07c0629f44935bbac07495789bba6",
            "b259c5afd61c482da7676e279df0f587",
            "4d5664d6ca874b48a58527adf8e30bf8"
          ]
        },
        "id": "c2mNwwx7SyNu",
        "outputId": "e1085094-5f86-4be2-fa8b-fd8e6ad9f018"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ded07c0629f44935bbac07495789bba6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b259c5afd61c482da7676e279df0f587",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d5664d6ca874b48a58527adf8e30bf8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7ea9aed341af83350e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://7ea9aed341af83350e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ================================================================\n",
        "# 2ã‚³ãƒç›®ãƒ»æ”¹ï¼šAIã®æ€è€ƒã‚’è¦—ãï¼ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰©ä½“èªè­˜\n",
        "# ================================================================\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—1ï¼†2ï¼šæº–å‚™ï¼ˆåˆå›ã¨åŒã˜ï¼‰\n",
        "# ----------------------------------------------------------------\n",
        "!pip install transformers torch gradio --quiet\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—3ï¼šAIãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ï¼ˆåˆå›ã¨åŒã˜ï¼‰\n",
        "# ----------------------------------------------------------------\n",
        "image_classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—4ï¼šAIã®ã€Œæ€è€ƒã€ã‚’å¯è¦–åŒ–ã™ã‚‹ã€Œå‘½ä»¤ã€ã‚’ä½œã‚‹\n",
        "# ----------------------------------------------------------------\n",
        "# äºˆæ¸¬çµæœã®ãƒªã‚¹ãƒˆã‚’ã€Gradioã®Labelã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæ‰±ãˆã‚‹å½¢å¼ï¼ˆè¾æ›¸ï¼‰ã«å¤‰æ›ã™ã‚‹\n",
        "# ----------------------------------------------------------------\n",
        "def classify_image_realtime(image):\n",
        "    # ç”»åƒãŒå…¥åŠ›ã•ã‚Œã¦ã„ãªã„å ´åˆã¯Noneã‚’è¿”ã™\n",
        "    if image is None:\n",
        "        return None\n",
        "\n",
        "    predictions = image_classifier(image)\n",
        "\n",
        "    # äºˆæ¸¬çµæœã‚’ {ãƒ©ãƒ™ãƒ«å: ç¢ºä¿¡åº¦ã‚¹ã‚³ã‚¢} ã®è¾æ›¸å½¢å¼ã«å¤‰æ›ã™ã‚‹\n",
        "    # ä¸Šä½3ä»¶ã ã‘ã‚’å–ã‚Šå‡ºã™\n",
        "    result_dict = {p['label']: p['score'] for p in predictions[:10]}\n",
        "    return result_dict\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# ã‚¹ãƒ†ãƒƒãƒ—5ï¼šãƒ©ã‚¤ãƒ–ã‚«ãƒ¡ãƒ©å¯¾å¿œã®æ“ä½œãƒ‘ãƒãƒ«ã‚’èµ·å‹•ã™ã‚‹\n",
        "# ----------------------------------------------------------------\n",
        "# inputsã«Webã‚«ãƒ¡ãƒ©ã‚’æŒ‡å®šã—ã€live=True ã«è¨­å®šã™ã‚‹ã“ã¨ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å‡¦ç†ãŒå®Ÿç¾ã™ã‚‹\n",
        "# ----------------------------------------------------------------\n",
        "gr.Interface(fn=classify_image_realtime,\n",
        "             inputs=gr.Image(sources=[\"webcam\"], type=\"pil\", label=\"Webã‚«ãƒ¡ãƒ©\"),\n",
        "             outputs=gr.Label(num_top_classes=10, label=\"AIã®èªè­˜çµæœï¼ˆãƒˆãƒƒãƒ—10ï¼‰\"),\n",
        "             title=\"ğŸ‘€ AIã®æ€è€ƒã‚’è¦—ãï¼ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰©ä½“èªè­˜ã‚«ãƒ¡ãƒ©\",\n",
        "             description=\"Webã‚«ãƒ¡ãƒ©ã«èº«ã®å›ã‚Šã®ã‚‚ã®ã‚’å†™ã—ã¦ã¿ã¦ãã ã•ã„ã€‚AIãŒä½•ã‚’èªè­˜ã—ã¦ã„ã‚‹ã‹ã€ãã®ç¢ºä¿¡åº¦ã¨ä¸€ç·’ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§è¡¨ç¤ºã—ã¾ã™ã€‚\",\n",
        "             live=True).launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wG6JpuoiMzXS",
        "outputId": "658ccc1b-ce7e-4f76-a782-c8ba76f4e710"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n",
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AIãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\n",
            "\n",
            "ã‚‚ã— 'torch' ã‚„ 'torchvision' ãŒãªã„ã¨ã„ã†ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰ã€!pip install torch torchvision ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://72b6a96381540bf61a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://72b6a96381540bf61a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã™ã¹ã¦ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
        "import gradio as gr\n",
        "from PIL import Image, ImageFilter, ImageDraw\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.models import resnet50\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "\n",
        "# --- AIãƒ¢ãƒ‡ãƒ«ã¨ãƒ©ãƒ™ãƒ«ã®æº–å‚™ ---\n",
        "# ã“ã®æº–å‚™ã¯ä¸€åº¦ã ã‘è¡Œã„ã¾ã™\n",
        "try:\n",
        "    # Image Classification model\n",
        "    image_classifier = pipeline(\"image-classification\", model=\"google/vit-base-patch16-224\")\n",
        "    # ResNet50 for image analysis\n",
        "    model = resnet50(pretrained=True)\n",
        "    model.eval()\n",
        "\n",
        "    response = requests.get(\"https://git.io/JJkYN\")\n",
        "    labels = response.text.split(\"\\n\")\n",
        "    print(\"AIãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚\")\n",
        "except Exception as e:\n",
        "    print(f\"AIãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}\")\n",
        "    image_classifier = None\n",
        "    model = None\n",
        "    labels = None\n",
        "\n",
        "# --- 1. ç”»åƒåŠ å·¥ã®ãƒ­ã‚¸ãƒƒã‚¯ ---\n",
        "def image_processing(img, filter_type, blur_level, watermark_text):\n",
        "    if img is None:\n",
        "        return None, \"ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚\"\n",
        "    processed_img = img.copy()\n",
        "    if filter_type == \"ç™½é»’\":\n",
        "        processed_img = processed_img.convert('L')\n",
        "    elif filter_type == \"90åº¦å›è»¢\":\n",
        "        processed_img = processed_img.rotate(90, expand=True)\n",
        "    if blur_level > 0:\n",
        "        processed_img = processed_img.filter(ImageFilter.GaussianBlur(radius=blur_level))\n",
        "    if watermark_text:\n",
        "        draw = ImageDraw.Draw(processed_img)\n",
        "        draw.text((10, 10), watermark_text, fill=\"white\", stroke_width=1, stroke_fill=\"black\")\n",
        "    return processed_img, f\"å‡¦ç†å®Œäº†ï¼š{filter_type}\"\n",
        "\n",
        "# --- 2. AIã«ã‚ˆã‚‹ç”»åƒåˆ†æã®ãƒ­ã‚¸ãƒƒã‚¯ ---\n",
        "@torch.no_grad()\n",
        "def predict(input_img):\n",
        "    if model is None or labels is None or input_img is None:\n",
        "        return \"AIãƒ¢ãƒ‡ãƒ«ãŒæº–å‚™ã§ãã¦ã„ãªã„ã‹ã€ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚\"\n",
        "\n",
        "    img = F.resize(input_img, (224, 224))\n",
        "    img = F.to_tensor(img)\n",
        "    if img.shape[0] == 1: # ç™½é»’ç”»åƒã®å ´åˆ\n",
        "        img = img.repeat(3, 1, 1)\n",
        "    img = F.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "    output = model(img).squeeze(0)\n",
        "    probs = torch.nn.functional.softmax(output, dim=0).numpy()\n",
        "\n",
        "    return {labels[i]: float(probs[i]) for i in range(1000)}\n",
        "\n",
        "# --- 3. AIã®ã€Œæ€è€ƒã€ã‚’å¯è¦–åŒ–ã™ã‚‹ã€Œå‘½ä»¤ã€ã‚’ä½œã‚‹ (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰©ä½“èªè­˜) ---\n",
        "def classify_image_realtime(image):\n",
        "    if image_classifier is None or image is None:\n",
        "        return \"AIãƒ¢ãƒ‡ãƒ«ãŒæº–å‚™ã§ãã¦ã„ãªã„ã‹ã€ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚\"\n",
        "\n",
        "    predictions = image_classifier(image)\n",
        "\n",
        "    result_dict = {p['label']: p['score'] for p in predictions[:10]}\n",
        "    return result_dict\n",
        "\n",
        "# --- Gradioã®UIã‚’æ§‹ç¯‰ ---\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# ç·åˆç”»åƒå‡¦ç†ã‚¢ãƒ—ãƒª\")\n",
        "    gr.Markdown(\"æ‰‹å‹•ã§ã®ç”»åƒåŠ å·¥ã€AIã«ã‚ˆã‚‹ç”»åƒåˆ†æã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰©ä½“èªè­˜ãŒã§ãã¾ã™ã€‚\")\n",
        "\n",
        "    with gr.Tabs():\n",
        "        # --- ã‚¿ãƒ–1: ç”»åƒåŠ å·¥ ---\n",
        "        with gr.TabItem(\"ç”»åƒåŠ å·¥\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    proc_input = gr.Image(type=\"pil\", label=\"åŠ å·¥ã—ãŸã„ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\")\n",
        "                    filter_radio = gr.Radio([\"ãªã—\", \"ç™½é»’\", \"90åº¦å›è»¢\"], label=\"ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’é¸æŠ\", value=\"ãªã—\")\n",
        "                    blur_slider = gr.Slider(0, 10, value=0, label=\"ã¼ã‹ã—ã®å¼·ã•\")\n",
        "                    watermark_input = gr.Textbox(label=\"é€ã‹ã—æ–‡å­—\")\n",
        "                    proc_btn = gr.Button(\"ç”»åƒå‡¦ç†ã‚’å®Ÿè¡Œ\")\n",
        "                with gr.Column():\n",
        "                    proc_output = gr.Image(label=\"åŠ å·¥å¾Œã®ç”»åƒ\")\n",
        "                    proc_status = gr.Textbox(label=\"å‡¦ç†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹\")\n",
        "            proc_btn.click(\n",
        "                fn=image_processing,\n",
        "                inputs=[proc_input, filter_radio, blur_slider, watermark_input],\n",
        "                outputs=[proc_output, proc_status]\n",
        "            )\n",
        "\n",
        "        # --- ã‚¿ãƒ–2: AIç”»åƒåˆ†æ ---\n",
        "        with gr.TabItem(\"AIç”»åƒåˆ†æ\"):\n",
        "            with gr.Row():\n",
        "                with gr.Column():\n",
        "                    ai_input = gr.Image(type=\"pil\", label=\"åˆ†æã—ãŸã„ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\")\n",
        "                    ai_btn = gr.Button(\"AIåˆ†æã‚’å®Ÿè¡Œ\")\n",
        "                with gr.Column():\n",
        "                    ai_output = gr.Label(num_top_classes=5, label=\"åˆ†æçµæœ\")\n",
        "            ai_btn.click(\n",
        "                fn=predict,\n",
        "                inputs=ai_input,\n",
        "                outputs=ai_output\n",
        "            )\n",
        "\n",
        "        # --- ã‚¿ãƒ–3: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰©ä½“èªè­˜ ---\n",
        "        with gr.TabItem(\"ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰©ä½“èªè­˜\"):\n",
        "             with gr.Row():\n",
        "                 with gr.Column():\n",
        "                     realtime_input = gr.Image(sources=[\"webcam\"], type=\"pil\", label=\"Webã‚«ãƒ¡ãƒ©\")\n",
        "                     realtime_btn = gr.Button(\"AIåˆ†æã‚’å®Ÿè¡Œ (ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ )\") # ãƒœã‚¿ãƒ³ã¯ä¸è¦ã ãŒã€UIã‚’åˆã‚ã›ã‚‹ãŸã‚ã«æ®‹ã™\n",
        "                 with gr.Column():\n",
        "                     realtime_output = gr.Label(num_top_classes=10, label=\"AIã®èªè­˜çµæœï¼ˆãƒˆãƒƒãƒ—10ï¼‰\")\n",
        "             # Use change() for real-time updates from webcam\n",
        "             realtime_input.change(\n",
        "                 fn=classify_image_realtime,\n",
        "                 inputs=realtime_input,\n",
        "                 outputs=realtime_output,\n",
        "             )\n",
        "\n",
        "\n",
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ä¿ƒã™ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
        "print(\"\\nã‚‚ã— 'torch' ã‚„ 'torchvision' ãŒãªã„ã¨ã„ã†ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰ã€!pip install torch torchvision ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
        "# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®èµ·å‹•\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 761
        },
        "id": "p99xOJu6BCe8",
        "outputId": "6dc9bf9c-888e-4a7e-b95f-28ea05aa3774"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AIãƒ¢ãƒ‡ãƒ«ã®æº–å‚™OKï¼\n",
            "ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¾ã™ã€‚URLãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://a6ecd06fab5fc0e1ee.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a6ecd06fab5fc0e1ee.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™\n",
        "# gradio: Web UIã‚’ç°¡å˜ã«ä½œã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "# PIL (Pillow): ç”»åƒã‚’åŠ å·¥ã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "# numpy: æ•°å€¤è¨ˆç®—ã€ç‰¹ã«AIã§ã‚ˆãä½¿ã‚ã‚Œã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "# ãã®ä»–: AIãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã™ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
        "import gradio as gr\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import requests\n",
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "import torchvision.transforms.functional as F\n",
        "\n",
        "# --- ã“ã“ã‹ã‚‰AIã®æº–å‚™ (ã“ã®éƒ¨åˆ†ã¯å®Œæˆã—ã¦ã„ã¾ã™) ---\n",
        "# æˆæ¥­ã§ä½¿ã£ãŸResNet50ãƒ¢ãƒ‡ãƒ«ã¨ImageNetãƒ©ãƒ™ãƒ«ã‚’æº–å‚™ã—ã¾ã™\n",
        "# ã“ã®éƒ¨åˆ†ã¯ä¸€åº¦ã ã‘å®Ÿè¡Œã•ã‚Œã¾ã™\n",
        "try:\n",
        "    model = resnet50(pretrained=True)\n",
        "    model.eval()\n",
        "    response = requests.get(\"https://git.io/JJkYN\")\n",
        "    labels = response.text.split(\"\\n\")\n",
        "    print(\"AIãƒ¢ãƒ‡ãƒ«ã®æº–å‚™OKï¼\")\n",
        "except Exception as e:\n",
        "    print(\"AIãƒ¢ãƒ‡ãƒ«ã®æº–å‚™ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ:\", e)\n",
        "    model = None\n",
        "    labels = None\n",
        "# --- AIã®æº–å‚™ã“ã“ã¾ã§ ---\n",
        "\n",
        "\n",
        "# --- é–¢æ•°ã‚’å®šç¾©ã™ã‚‹ã‚¨ãƒªã‚¢ ---\n",
        "\n",
        "# ã€èª²é¡Œï¼‘ã€‘ç”»åƒåŠ å·¥ç”¨ã®é–¢æ•°ã‚’å®Œæˆã•ã›ã‚ˆã†ï¼\n",
        "# 1ã‚³ãƒç›®ã®æœ€å¾Œã«ä½œã£ãŸã€Œç™½é»’ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã€ã®å‡¦ç†ã‚’æ€ã„å‡ºã—ã¦æ›¸ã„ã¦ã¿ã‚ˆã†ï¼\n",
        "def image_filter(input_img):\n",
        "    print(\"ç”»åƒåŠ å·¥ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¾ã—ãŸï¼\")\n",
        "    if input_img is None:\n",
        "        return None # ç”»åƒãŒãªã‘ã‚Œã°ä½•ã‚‚ã—ãªã„\n",
        "\n",
        "    # â–¼â–¼â–¼ ã“ã“ã«ã€Pillowã‚’ä½¿ã£ãŸç”»åƒå‡¦ç†ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã‚ˆã† â–¼â–¼â–¼\n",
        "    # ãƒ’ãƒ³ãƒˆ: input_img.convert('L') ã‚’ä½¿ã†ã¨ç™½é»’ã«ã§ãã¾ã™\n",
        "\n",
        "    processed_img = input_img.convert('L') # ä»Šã¯ã¾ã ä½•ã‚‚ã—ãªã„ã®ã§ã€ã“ã®è¡Œã‚’æ›¸ãæ›ãˆã‚ˆã†ï¼\n",
        "\n",
        "    # â–²â–²â–² ã“ã“ã¾ã§ â–²â–²â–²\n",
        "\n",
        "    return processed_img\n",
        "\n",
        "\n",
        "# ã€èª²é¡Œï¼’ã€‘AIåˆ†æç”¨ã®é–¢æ•°ã‚’å®Œæˆã•ã›ã‚ˆã†ï¼\n",
        "# 2ã‚³ãƒç›®ã«ä½“é¨“ã—ãŸã€ŒAIã«ã‚ˆã‚‹ç”»åƒåˆ¤å®šã€ã®ã‚³ãƒ¼ãƒ‰ã‚’å‚è€ƒã«ã€ä¸­èº«ã‚’å®Œæˆã•ã›ã‚ˆã†ï¼\n",
        "def predict_image(input_img):\n",
        "    print(\"AIåˆ†æãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¾ã—ãŸï¼\")\n",
        "    if model is None or labels is None or input_img is None:\n",
        "        return \"AIãƒ¢ãƒ‡ãƒ«ãŒæº–å‚™ã§ãã¦ã„ãªã„ã‹ã€ç”»åƒãŒã‚ã‚Šã¾ã›ã‚“ã€‚\"\n",
        "\n",
        "    # â–¼â–¼â–¼ ã“ã“ã«ã€AIã«ç”»åƒã‚’åˆ†æã•ã›ã‚‹ã‚³ãƒ¼ãƒ‰ã‚’è¿½åŠ ã—ã‚ˆã† â–¼â–¼â–¼\n",
        "    # ãƒ’ãƒ³ãƒˆ: æ•™æã€3.Gradio(Webã‚¤ãƒ³ã‚¿ãƒ•ã‚§ãƒ¼ã‚¹)ã®åŸºæœ¬.ipynbã€ã®æœ€å¾Œã®éƒ¨åˆ†ã‚’å‚è€ƒã«ã—ã‚ˆã†\n",
        "    # æ‰‹é †ï¼‘ï¼šç”»åƒã®å‰å‡¦ç† (ãƒªã‚µã‚¤ã‚ºã€Tensorã«å¤‰æ›ã€æ­£è¦åŒ–)\n",
        "    # æ‰‹é †ï¼’ï¼šAIã«ã‚ˆã‚‹æ¨è«– (model(img) ã‚’å‘¼ã³å‡ºã™)\n",
        "    # æ‰‹é †ï¼“ï¼šçµæœã‚’æ•´å½¢ã—ã¦ã€returnã§è¿”ã™\n",
        "    img = F.resize(input_img, (224, 224))\n",
        "    img = F.to_tensor(img)\n",
        "    if img.shape[0] == 1: # ç™½é»’ç”»åƒã®å ´åˆ\n",
        "        img = img.repeat(3, 1, 1)\n",
        "    img = F.normalize(img, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    img = img.unsqueeze(0)\n",
        "\n",
        "    # æ¨è«–ã®å®Ÿè¡Œ\n",
        "    with torch.no_grad():\n",
        "        output = model(img).squeeze(0)\n",
        "    probs = torch.nn.functional.softmax(output, dim=0).numpy()\n",
        "\n",
        "    # çµæœã‚’{ãƒ©ãƒ™ãƒ«: ç¢ºç‡}ã®è¾æ›¸å½¢å¼ã§è¿”ã™\n",
        "    result = {labels[i]: float(probs[i]) for i in range(1000)} # ä»Šã¯ã¾ã ãƒ€ãƒŸãƒ¼ã®ãƒ‡ãƒ¼ã‚¿ãªã®ã§ã€ã“ã®è¡Œã‚’æ›¸ãæ›ãˆã‚ˆã†ï¼\n",
        "\n",
        "    # â–²â–²â–² ã“ã“ã¾ã§ â–²â–²â–²\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# --- Web UIã‚’ä½œã‚‹ã‚¨ãƒªã‚¢ (ã“ã®éƒ¨åˆ†ã¯å®Œæˆã—ã¦ã„ã¾ã™) ---\n",
        "# gr.Blocks() ã‚’ä½¿ã†ã¨ã€ã‚ˆã‚Šè‡ªç”±ã«UIã‚’çµ„ã¿ç«‹ã¦ã‚‰ã‚Œã¾ã™\n",
        "with gr.Blocks() as demo:\n",
        "    # ã‚¢ãƒ—ãƒªå…¨ä½“ã®ã‚¿ã‚¤ãƒˆãƒ«\n",
        "    gr.Markdown(\"# ã‚ªãƒªã‚¸ãƒŠãƒ«ç”»åƒåŠ å·¥ï¼†AIåˆ†æã‚¢ãƒ—ãƒª\")\n",
        "\n",
        "    # gr.Tabs() ã§æ©Ÿèƒ½ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã‚¿ãƒ–ã‚’ä½œã‚Šã¾ã™\n",
        "    with gr.Tabs():\n",
        "        # 1ã¤ç›®ã®ã‚¿ãƒ–\n",
        "        with gr.TabItem(\"ç”»åƒåŠ å·¥ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼\"):\n",
        "            gr.Markdown(\"## å¥½ããªç”»åƒã‚’åŠ å·¥ã—ã¦ã¿ã‚ˆã†ï¼\")\n",
        "            with gr.Row():\n",
        "                # å·¦å´ã«é…ç½®ã™ã‚‹UIéƒ¨å“\n",
        "                with gr.Column():\n",
        "                    image_input_filter = gr.Image(type=\"pil\", label=\"ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\")\n",
        "                    process_button = gr.Button(\"åŠ å·¥ã™ã‚‹ï¼\")\n",
        "                # å³å´ã«é…ç½®ã™ã‚‹UIéƒ¨å“\n",
        "                with gr.Column():\n",
        "                    image_output_filter = gr.Image(label=\"åŠ å·¥å¾Œã®ç”»åƒ\")\n",
        "\n",
        "            # ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã‚‰ã€ã©ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã™ã‹è¨­å®š\n",
        "            process_button.click(fn=image_filter, inputs=image_input_filter, outputs=image_output_filter)\n",
        "\n",
        "        # 2ã¤ç›®ã®ã‚¿ãƒ–\n",
        "        with gr.TabItem(\"AIç”»åƒåˆ†æ\"):\n",
        "            gr.Markdown(\"## AIãŒå†™çœŸã«å†™ã£ã¦ã„ã‚‹ã‚‚ã®ã‚’å½“ã¦ã‚‹ã‚ˆï¼\")\n",
        "            with gr.Row():\n",
        "                # å·¦å´ã«é…ç½®ã™ã‚‹UIéƒ¨å“\n",
        "                with gr.Column():\n",
        "                    image_input_ai = gr.Image(type=\"pil\", label=\"ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰\")\n",
        "                    predict_button = gr.Button(\"AIã«åˆ†æã—ã¦ã‚‚ã‚‰ã†ï¼\")\n",
        "                # å³å´ã«é…ç½®ã™ã‚‹UIéƒ¨å“\n",
        "                with gr.Column():\n",
        "                    # gr.Labelã¯AIã®åˆ†æçµæœã‚’è¡¨ç¤ºã™ã‚‹ã®ã«ä¾¿åˆ©ã§ã™\n",
        "                    label_output_ai = gr.Label(num_top_classes=3, label=\"åˆ†æçµæœ\")\n",
        "\n",
        "            # ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚ŒãŸã‚‰ã€ã©ã®é–¢æ•°ã‚’å‘¼ã³å‡ºã™ã‹è¨­å®š\n",
        "            predict_button.click(fn=predict_image, inputs=image_input_ai, outputs=label_output_ai)\n",
        "\n",
        "\n",
        "# ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¾ã™\n",
        "print(\"ã‚¢ãƒ—ãƒªã‚’èµ·å‹•ã—ã¾ã™ã€‚URLãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚\")\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOxVC/VkwsU6japUamdflVt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}